{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "## Nick Boddy<br>nboddy\n",
    "\n",
    "Here I implement a decision tree \"from scratch\", using libraries only for convenience of data structuring and manipulation.\n",
    "\n",
    "This implementation assumes that the data consists of continuous numeric features and a classification label in {0, 1}.\n",
    "The tree makes splits using GainRatio. If it is determined that a leaf node contains no majority class, it defaults to \"1\".\n",
    "\n",
    "\n",
    "Below the implementation I address questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.    A Simplified Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.264185</td>\n",
       "      <td>0.178456</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.409499</td>\n",
       "      <td>0.213456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.926224</td>\n",
       "      <td>0.540329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.573685</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.953159</td>\n",
       "      <td>0.608121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2  y\n",
       "0  0.264185  0.178456  0\n",
       "1  0.409499  0.213456  1\n",
       "2  0.926224  0.540329  1\n",
       "3  0.573685  0.282145  1\n",
       "4  0.953159  0.608121  1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./data/D1.txt', sep='\\s+', names=['x1', 'x2', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_candidate_numeric_splits(D: pd.DataFrame, X: str) -> list((str, float)):\n",
    "    C = []\n",
    "    instances = D.sort_values(by=X)\n",
    "    instances = instances.reset_index(drop=True)\n",
    "    for i, row in instances.iterrows():\n",
    "        if i < len(instances) - 1:\n",
    "            y_i = row['y']\n",
    "            y_next = instances.at[i + 1, 'y']\n",
    "            x_next = instances.at[i + 1, X]\n",
    "            if y_i != y_next and (X, x_next) not in C:\n",
    "                C.append((X, x_next))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_candidate_splits(D: pd.DataFrame) -> list((str, float)):\n",
    "    C = []\n",
    "    for feature in D.columns[:-1]:\n",
    "        instances = D[:]\n",
    "        C.extend(determine_candidate_numeric_splits(instances, feature))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_Y(D: pd.DataFrame, Y: str) -> float:\n",
    "    # H(Y) = -Sum(P(y)*log2(P(y)) for each y in Y)\n",
    "    H = 0\n",
    "    y = D[Y].unique()\n",
    "    for y_i in y:\n",
    "        Py_i = sum(D[Y] == y_i) / len(D[Y])\n",
    "        H += Py_i if Py_i == 0 else Py_i * math.log2(Py_i)\n",
    "    H *= -1\n",
    "    return H\n",
    "\n",
    "def entropy_S(D: pd.DataFrame, S: (str, float)) -> float:\n",
    "    # H(S) = -(P(X < c)*log2(P(X < c) + P(X >= c)*log2(P(X >= c)))\n",
    "    # careful not to calculate log2(0)\n",
    "    H = 0\n",
    "    X, c = S\n",
    "    Pgreater = sum(D[X] >= c) / len(D[X])\n",
    "    H += Pgreater if Pgreater == 0 else Pgreater * math.log2(Pgreater)\n",
    "    Plesser = sum(D[X] < c) / len(D[X])\n",
    "    H += Plesser if Plesser == 0 else Plesser * math.log2(Plesser)\n",
    "    H *= -1\n",
    "    return H\n",
    "\n",
    "def conditional_entropy(D: pd.DataFrame, Y: str, S: (str, float)) -> float:\n",
    "    # H(Y | S) = (P(X >= c) * H(Y | X >= c)) + (P(X < c) * H(Y | X < c))\n",
    "    H = 0\n",
    "    X, c = S\n",
    "    HYgreater = entropy_Y(D[D[X] >= c], Y)\n",
    "    Pgreater = sum(D[X] >= c) / len(D[X])\n",
    "    HYlesser = entropy_Y(D[D[X] < c], Y)\n",
    "    Plesser = sum(D[X] < c) / len(D[X])\n",
    "    H += Pgreater * HYgreater\n",
    "    H += Plesser * HYlesser\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    feature: str = None\n",
    "    threshold: float = None\n",
    "    children: List['Node'] = None\n",
    "    class_label: float = None\n",
    "    layer: int = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        result = ''\n",
    "        if self.class_label != None:\n",
    "            result += 'y = ' + str(self.class_label)\n",
    "        elif self.feature and self.threshold:\n",
    "            result += self.feature + ' >= ' + str(self.threshold)\n",
    "        if self.children:\n",
    "            result += '\\n' + '\\t'*self.layer + '|\\n' + '\\t'*self.layer + '├──' + str(self.children[0])\n",
    "            result += '\\n' + '\\t'*self.layer + '|\\n' + '\\t'*self.layer + '└──' + str(self.children[1])\n",
    "            #result += '[' + str(self.children[0]) + ', ' + str(self.children[1]) + ']'\n",
    "        return result if result else '<???>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtree(D: pd.DataFrame, layer: int = 0) -> Node:\n",
    "    # print('BEGIN make_subtree')\n",
    "    # print('Data: \\n')\n",
    "    # print(D)\n",
    "    stop = False\n",
    "    best_split = None\n",
    "    candidate_splits = determine_candidate_splits(D[:])\n",
    "    # print('Candidate splits: \\n')\n",
    "    # print(candidate_splits)\n",
    "\n",
    "    # STOP if node is empty (or we have no splits)\n",
    "    if len(D) == 0 or len(candidate_splits) == 0:\n",
    "        stop = True\n",
    "    else:\n",
    "        gainRatios = []\n",
    "        for split in candidate_splits:\n",
    "            split_entropy = entropy_S(D, split)\n",
    "            if split_entropy == 0:\n",
    "                # found split with zero entropy (100% of the training instances are on one side)\n",
    "                # ignore this, so set gainRatio to zero\n",
    "                gainRatio = 0.0\n",
    "                gainRatios.append(gainRatio)\n",
    "            else:\n",
    "                # GainRatio(D, Split) = (H(Y) - H(Y | Split)) / H(Split)\n",
    "                gainRatio = (entropy_Y(D, 'y') - conditional_entropy(D, 'y', split)) / entropy_S(D, split)\n",
    "                gainRatios.append(gainRatio)\n",
    "        gainRatios = pd.Series(gainRatios)\n",
    "        # STOP if all splits give 0 info (gainRatio)\n",
    "        if len(gainRatios) == 0 or gainRatios.max() == 0:\n",
    "            stop = True\n",
    "        else:\n",
    "            best_split = candidate_splits[gainRatios.idxmax()]\n",
    "    if stop:\n",
    "        # stopping criteria met\n",
    "        # stop when there are no training instances, or \n",
    "        majority_class = D['y'].mode()\n",
    "        if len(majority_class) > 1:\n",
    "            majority_class = '1'\n",
    "        else:\n",
    "            majority_class = majority_class[0]\n",
    "        # print(str(sum(D['y'] == majority_class)) + ' / ' + str(len(D)) + ' = ' + str(majority_class))\n",
    "        new_node = Node(class_label=majority_class, layer=layer)\n",
    "        return new_node\n",
    "    else:\n",
    "        # make internal node and make children subtrees\n",
    "        feature, threshold = best_split\n",
    "        left_data = D[D[feature] >= threshold]\n",
    "        right_data = D[D[feature] < threshold]\n",
    "        # print('split: ' + str(len(left_data)) + ' & ' + str(len(right_data)))\n",
    "        left_child = make_subtree(left_data, layer + 1)\n",
    "        right_child = make_subtree(right_data, layer + 1)\n",
    "        new_node = Node(feature, threshold, [left_child, right_child], layer=layer)\n",
    "        return new_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.    Questions\n",
    "\n",
    "1. If a node is not empty but contains training items with the same label, why is it guaranteed to become a leaf? Explain. You may assume that the feature values of these items are not all the same.\n",
    "\n",
    "If a node contains only training items of one label, say \"1\", it will always become a leaf because the algorithm will not produce any candidate splits (since when comparing every pair of adjacent feature values, they don't differ in label class).\n",
    "\n",
    "Even in the case where my implementation differed and created candidate splits at every feature point, every candidate split would have either zero gain ratio or zero entropy, which triggers our stopping criteria.\n",
    "\n",
    "\n",
    "2. Handcraft a small training set where both classes are present but the algorithm refuses to split; instead it makes the root a leaf and stop; Importantly, if we were to manually force a split, the algorithm will happily continue splitting the data set further and produce a deeper tree with zero training error. You should (1) plot your training set, (2) explain why. Hint: you don’t need more than a handful of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  1\n",
       "2   1   0  1\n",
       "3   1   1  0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_df = pd.DataFrame([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0]])\n",
    "q2_df.columns = ['x1', 'x2', 'y']\n",
    "q2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4 = 1\n",
      "y = 1\n"
     ]
    }
   ],
   "source": [
    "q2_dtree = make_subtree(q2_df)\n",
    "print(q2_dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe above, the decision tree made on the 4-row training data (q2_df), the tree consists of just a root node with class label y = 1.\n",
    "\n",
    "This dataset doesn't allow the tree to split because it's designed in such a way that a split on any feature keeps P(y=0) equal to P(y=1). The dataset follows y:= x1 xor x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEYCAYAAAAeWvJ8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUNElEQVR4nO3dfZBeZXnH8e+VJQGUIGpWSwMU2ont0KIWFuK0dqqllKB20mptEUdHfIkI8aX/FFpb27FObXU6rQ5gumKGgWGaob7UaIOZqqNOq9RsWkECohkssATLRhjkLSS7e/WP59n4sGyW51nO4T7nyfczcyZ7Xvac++SP/c11n/vcJzITSZJKWVa6AZKkw5tBJEkqyiCSJBVlEEmSijKIJElFGUSSpKIMIkk6DEXE5oi4LyJuOcT+iIiPR8TuiLg5Ik6vqy0GkSQdnq4G1i2y/zxgTXfZAHyiroYYRJJ0GMrMbwD3L3LIeuCa7LgROC4ijq+jLUfUcdI6rVq1Kk8++eTSzZCkRe3cuXNvZo5Wca5zX/ns/PH9M/1f++bHdwH7ejaNZ+b4gJddDdzdsz7Z3XbvgOd5Sq0LopNPPpmJiYnSzZCkRUXEnVWd68f3z/Dt7Sf1ffzI8T/Yl5ljT/OyscC2WuaEa10QSdLhJoFZZp/py04CJ/asnwDsqeNCPiOSpMZLZnK276UiW4E3d0fPvQx4MDMr75YDKyJJarxORVRtr1hE/DPwCmBVREwCfwksB8jMTcA24FXAbuBR4MJKG9DDIJKkFqi6ay4z3/AU+xO4pNKLHoJBJEkNlyQzQ/ztOINIklqg6q65JjGIJKnhEpgxiAYXEZuB1wD3ZeavLLA/gI/ReRj2KPCWzPzvKtuQmeRjn4fHvwTLnk8cczExsrrKS0gaUg/u/QnXfvBf+L//neJXzz6N33v3eSxbVm6gsRXR0lwNXA5cc4j9vfMYraUzj9HaKhuQj1wFD18OPAYsI/dth1XbiJEXVHkZSUPmsYcf4+KxS7n/3geYPjDD/3z1Fu66bZL3bXpnkfYkDPUzotrivRHzGD0yTieEAGYh98G+L1Z6CUnDZ2L7TTx0/8NMH+hMq/P4o49zw6e+yoH9B4q1aXaApW1KvtB6qHmMniQiNkTERERMTE1NDXCJ+XMzzUL2P1+TpMPTzPTCfydytkxVkiQzAyxtUzKI+p7HKDPHM3MsM8dGRweYQ/Do1wNH9VxxBRx17mCtlHTYOf23X8zyI5ezbFnnz9SKo1fwstecwYqjVpRpUMLMAEvblBw1V/s8RrHyT8hlz4F922HZccTKS4kj+p84UNLh6djnr+Ty//owV7x3M/fdtZfTzz6Nt374jcXa05lZYXiVDKKtwMaI2EJnkELl8xhFjBDHXAzHXFzlaSUdBo7/+RfyoS/8aelmdAUzC3YiDYc6h283Zh4jSWqzBAo9nnpG1BZETZrHSJLazopIklRMZ2YFg0iSVNBsGkSSpEKsiCRJRSXBzBB/UNsgkqQWsGtOklSMXXOSpMKCmbRrTpJUSGeKH4NIklSQXXOSpGIy7ZqTJBU2a0UkSSqlM2rOikiSVIxdc5Kkghw1J0kqbsaZFSRJpTjXnCSpuFmfEUmSSnHUnCSpqCR8RiRJKstRc5KkYjLxPSJJUknhFD+SpHISKyJJUmGOmpMkFZMEs46akySVZEUkSSomcWYFSVJR4afCJUnlWBFJkoob5opoeCNWkoZEZjCby/pe+hER6yLi9ojYHRGXLbD/ORHxhYi4KSJ2RcSFld9YlxWRJLVAlS+0RsQIcAVwDjAJ7IiIrZl5a89hlwC3ZubvRsQocHtEXJeZ+ytrSJcVkSQ1XOdT4dH30oezgN2ZeUc3WLYA6xe47MqICOAY4H5gusLbOsiKSJIaL6qe4mc1cHfP+iSwdt4xlwNbgT3ASuCPMnO2ykbMsSKSpIbrjJqLvhdgVURM9Cwb5p1yobIp562fC3wH+FngpcDlEXFstXfWYUUkSS0w4MwKezNzbJH9k8CJPesn0Kl8el0I/G1mJrA7In4I/BLw7UEa0g8rIklquLm55gaoiJ7KDmBNRJwSESuA8+l0w/W6CzgbICJeCPwicEeFt3VQrUHUpOGBktRmsyzre3kqmTkNbAS2A7cB12fmroi4KCIu6h7218CvRcR3ga8Al2bm3jrurbauuaYND5Sktup8obXaF1ozcxuwbd62TT0/7wF+p9KLHkKdz4gODg8EiIi54YG9QfSMDQ+UpDbzMxBL06jhgZLUVp1nRMP7SL/OO6tseGBEbJgbhjg1NVV1OyWp8Wa6M3D3s7RNnUHU7/DAz2bHbmBueOATZOZ4Zo5l5tjo6GhtDZakJlrCe0StUmcQNWp4oCS1V/WTnjZJbc+IMnM6IuaGB44Am+eGB3b3b6IzPPDq7vDAoMbhgZLUZn3OIddKtc6s0KThgZLUVnUM324Sp/iRpBZoY5dbvwwiSWq4uSl+hpVBJEkt4DMiSVIxc8O3h5VBJEkt4DMiSVI5LX1RtV8GkSQ1XOIzIklSYVZEkqRiHKwgSSrOIJIkFeMLrZKk4hysIEkqJ+2akyQV5GAFSVJxBpEkqRgHK0iSikuDSJJUkqPmJEnFpKPmJEml2TUnSSrIwQqSpMKsiCRJxfhCqySprOwMWBhWBpEktYDDtyVJxSQ+I5IkFeWoOUlSYT4jkiQVZdecJKmYTINIklSYz4gkSUX5jEiSVJRdc5KkYpIwiCRJZQ1xz5xBJEmNN+Sj5paVboAkqQ85wNKHiFgXEbdHxO6IuOwQx7wiIr4TEbsi4utP/yYWZkUkSS1QZUUUESPAFcA5wCSwIyK2ZuatPcccB1wJrMvMuyLiBZU1YJ5aK6ImJa4ktVlm/0sfzgJ2Z+Ydmbkf2AKsn3fMBcBnM/OuzvXzvirvp1dtFVHTEleS2moJs2+vioiJnvXxzBzvWV8N3N2zPgmsnXeOFwHLI+JrwErgY5l5zSCN6FedXXMHExcgIuYS99aeY56xxJWk1kpgsCDam5lji+xf6GTza6kjgDOAs4GjgW9FxI2Z+f1BGtKPOrvmFkrc1fOOeRHw3Ij4WkTsjIg3L3SiiNgQERMRMTE1NVVTcyWpuSrumpsETuxZPwHYs8AxX8rMRzJzL/AN4CVV3Mt8dQbRIIn7auBc4C8i4kVP+qXM8cwcy8yx0dHR6lsqSU1X7ai5HcCaiDglIlYA5wNb5x3zeeA3IuKIiHgWna67257+jTxZnV1z/Sbu3sx8BHgkIuYSt/LST5Laq9qZFTJzOiI2AtuBEWBzZu6KiIu6+zdl5m0R8SXgZmAWuCozb6msET3qDKKDiQvcQydxL5h3zOeByyPiCGAFncT9hxrbJEntVPHUCpm5Ddg2b9umeesfBT5a7ZWfrLYgalriSlJrDfnMCrW+0NqkxJWkVhviyeacWUGSWsGKSJJUkhWRJKkog0iSVMzgMyu0ikEkSS3Q54wJrWQQSVIbGESSpKLsmpMklRRWRJKkYgb4BHgbGUSS1Hhh15wkqTArIklSUQaRJKkog0iSVIwzK0iSShvm4dvLFtsZEcdGxC8ssP3F9TVJkvQkOcDSMocMooj4Q+B7wGciYldEnNmz++q6GyZJao+I2BgRz13K7y5WEf0ZcEZmvhS4ELg2Il47d82lXEyStDSR/S+F/AywIyKuj4h1EdF3TiwWRCOZeS9AZn4beCXw/oh4D60s/iSpxTL6X0o0L/PPgTXAp4C3AD+IiL9Z6PHOfIsF0UO9J+iG0iuA9cAvP50GS5IGMMjzoYJlQmYm8KPuMg08F/h0RHxksd9bLIjeBSyLiFN7LvIQsA54+9NusSSpfw0Pooh4T0TsBD4C/CdwWma+CzgDeN1iv3vI4duZeVP35LdExLXdkx/V/XcMuLaa5kuSnkoLhm+vAl6bmXf2bszM2Yh4zWK/uOjw7a61wInAN4EdwB7g15fYUEnSUjS8IsrMD8wPoZ59ty32u/280HoAeAw4mk5F9MPMnB24lZKkpWt+RbRk/VREO+gE0ZnAy4E3RMSna22VJOmgQYZut6AL70n6qYjelpkT3Z9/BKyPiDfV2CZJ0nyH81xzPSHUu82BCpL0TGphpdMvJz2VpBZoY5dbvwwiSWoDg0iSVExLByH0yyCSpDYwiCRJRRlEkqSShrlrrp8XWiVJqo0VkSS1wRBXRAaRJDWdo+YkScUZRJKkooY4iGodrBAR6yLi9ojYHRGXLXLcmRExExF/UGd7JKmNguGefbu2IIqIEeAK4DzgVDqfjzj1EMf9HbC9rrZIUus1/MN4T0edFdFZwO7MvCMz9wNbgPULHPdu4DPAfTW2RZLaq4bvETWpx6rOIFoN3N2zPtnddlBErAZ+H9i02IkiYkNETETExNTUVOUNlaTGq7AialqPVZ1BtNBXnOb/F/0jcGlmzix2oswcz8yxzBwbHR2tqn2S1B7Vds01qseqzlFzk8CJPesnAHvmHTMGbIkIgFXAqyJiOjP/tcZ2SVLrDDgIYVVE9H7UdDwzx3vWF+qxWvuE6/20x+q3gDMHuvqA6gyiHcCaiDgFuAc4H7ig94DMPGXu54i4GviiISRJCxgsiPZm5tgi+wfqseoWC7WpLYgyczoiNtLpWxwBNmfmroi4qLt/0edCkqSu6kfDNarHqtYXWjNzG7Bt3rYFAygz31JnWySpzSp+P6hRPVbOrCBJbVBhEDWtx8ogkqQWqHrGhCb1WBlEktQGLZwxoV8GkSQ1XUun7umXQSRJDRcsPN56WBhEktQGVkSSpJLa+HmHfhlEktQGBpEkqSiDSJJUTEu/vNovg0iS2sAgkiSVZEUkSSrLIJIklWRFJEkqxyl+JEnFGUSSpFICu+YkSaUZRJKkkiKHN4kMIklqOgcrSJJK8xmRJKksg0iSVJIVkSSpLINIklSMn4GQJBVnEEmSSnFmBUlSeb7QKkkqyYpIklSOMytIkkqL2dItqI9BJEltYEUkSSrJZ0SSpHISR81JksqyIpIklWUQSZJKGfaZFZbVefKIWBcRt0fE7oi4bIH9b4yIm7vLNyPiJXW2R5JaKXOwpWVqq4giYgS4AjgHmAR2RMTWzLy157AfAr+ZmQ9ExHnAOLC2rjZJUltZES3NWcDuzLwjM/cDW4D1vQdk5jcz84Hu6o3ACTW2R5LaKwdYWqbOIFoN3N2zPtnddihvA26osT2S1FqR/S9tU+dghVhg24L/RRHxSjpB9PJD7N8AbAA46aSTqmqfJLVDArMtTJg+1VkRTQIn9qyfAOyZf1BEvBi4ClifmT9e6ESZOZ6ZY5k5Njo6WktjJanR7Jpbkh3Amog4JSJWAOcDW3sPiIiTgM8Cb8rM79fYFklqtWHumqstiDJzGtgIbAduA67PzF0RcVFEXNQ97APA84ErI+I7ETFRV3skqdUqHr7dpNdran2hNTO3AdvmbdvU8/PbgbfX2QZJGgZVVjpNe72m1hdaJUkVGOT5UH+B1ajXa5ziR5IarjPFT6UPfxZ6vWaxaqfW12sMIklqg8G+0Lpq3jP38cwc71mv7PWaKhhEktQCA1ZEezNzbJH9g75ec96hXq+pgs+IJKnpqn9G1KjXa6yIJKnxqp1VOzOnI2Lu9ZoRYPPc6zXd/Zt44us1ANNPUWUtmUEkSS1Q9YuqTXq9xiCSpDZo4XeG+mUQSVLTJcRgo+ZaxSCSpDawIpIkFTW8OWQQSVIbVDyzQqMYRJLUBgaRJKmYZNApflrFIJKkhgvSrjlJUmEGkSSpKINIklSMz4gkSaX5jEiSVJZBJEkqp9rPQDSNQSRJTZcYRJKkwhysIEkqycEKkqSyDCJJUjEJzBpEkqRiHDUnSSrNIJIkFWUQSZKK8RmRJKmshBzeF4kMIklqA7vmJEnF2DUnSSrOikiSVJRBJEkqxxdaJUklJTDrqDlJUklDXBEtq/PkEbEuIm6PiN0RcdkC+yMiPt7df3NEnF51G3b++01c/u6ruOaD1/Pg3p9UfXpJQypnH2X24U8y++BfkftuIEsHQWb/S8vUVhFFxAhwBXAOMAnsiIitmXlrz2HnAWu6y1rgE91/K3HD5q9wxXs28/ij+xlZPsK//dOX+eR3/55jn7eyqktIGkKZ+8n7Xw/TdwL7yX2fg2d9j1j5x6VaNNTDt+usiM4CdmfmHZm5H9gCrJ93zHrgmuy4ETguIo6vqgGfuuw6Hn90PwAzB2Z4+IGH+ep1/1HV6SUNq8e/DjP3AJ2/H+Rj8Mg4mdNl2pOQOdv30jZ1BtFq4O6e9cnutkGPISI2RMRERExMTU313YD9+w48YX1mepZ9j+zr+/clHaZyX2eAwJMcWGjjM2M2+19aps4gigW2zf8f6ucYMnM8M8cyc2x0dLTvBrz8dWtZcfSKg+tHrBhh7asrfwwladisOAui98/TClh+OhFHF2uSz4iWZhI4sWf9BGDPEo5ZsvdteidHHn0k39q6g2OOezaXfPytnHLaz1V1eklDKkZeCM+7jnzw/TB7HywfI57zoXINynT49hLtANZExCnAPcD5wAXzjtkKbIyILXQGKTyYmfdW1YAVRy7nvVe+g/de+Y6qTinpMBHLTyVWfa50M36qhZVOv2oLosycjoiNwHZgBNicmbsi4qLu/k3ANuBVwG7gUeDCutojSW2WVkRLk5nb6IRN77ZNPT8ncEmdbZCk9mvns59+ObOCJDWdn4GQJBXXwveD+mUQSVLDJZBDXBHVOtecJKkCmZ2KqN+lD02YC3SOFZEktUCVFVET5gLtZUUkSW1QbUVUfC7QXq2riHbu3Lk3Iu5cwq+uAvZW3Z4G8L7aZ1jvzft6osqmcXmIB7Z/OT+9aoBfOSoiJnrWxzNzvGd9oXk+51c7h5oLtLJJB+a0Logys//J5npExERmjlXdntK8r/YZ1nvzvuqTmesqPmVlc4FWwa45STr8FJ8LtJdBJEmHn4NzgUbECjpzgW6dd8xW4M3d0XMvo+K5QHu1rmvuaRh/6kNayftqn2G9N++rJZo2F2gU/w67JOmwZtecJKkog0iSVJRBJEkqyiCSJBVlEEmSijKIJElFGUSSpKL+HxhsByeszyIHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = q2_df.plot.scatter(x='x1', y='x2', c='y', colormap='viridis')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the plot of q2_df. Yellow indicates the point has classification label 1, and purple 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the training set Druns.txt. For the root node, list all candidate cuts and their information gain ratio. If the entropy of the candidate split is zero, please list its mutual information (i.e. information gain). Hint: to get log2(x) when your programming language may be using a different base, use log(x)/log(2). Also, please follow the split rule in the first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1  x2  y\n",
       "0   0.1  -2  0\n",
       "1   0.0  -1  1\n",
       "2   0.0   0  0\n",
       "3   0.0   1  0\n",
       "4   0.0   2  0\n",
       "5   0.0   3  0\n",
       "6   0.0   4  0\n",
       "7   0.0   5  0\n",
       "8   0.0   6  1\n",
       "9   0.0   7  0\n",
       "10  0.0   8  1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "druns_df = pd.read_csv('./data/Druns.txt', sep='\\s+', names=['x1', 'x2', 'y'])\n",
    "druns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root candidate cuts: \n",
      "[x1 >= 0.0]: 0.0 (zero split entropy)\n",
      "[x1 >= 0.1]: 0.10051807676021852\n",
      "[x2 >= -1]: 0.10051807676021852\n",
      "[x2 >= 0]: 0.055953759631263686\n",
      "[x2 >= 6]: 0.2360996061436081\n",
      "[x2 >= 7]: 0.055953759631263686\n",
      "[x2 >= 8]: 0.43015691613098095\n"
     ]
    }
   ],
   "source": [
    "druns_root_splits = determine_candidate_splits(druns_df)\n",
    "ratios = []\n",
    "print('Root candidate cuts: ')\n",
    "for split in druns_root_splits:\n",
    "    HS = entropy_S(druns_df, split)\n",
    "    HY = entropy_Y(druns_df, 'y')\n",
    "    HYS = conditional_entropy(druns_df, 'y', split)\n",
    "    if HS == 0:\n",
    "        infoGain = HY - HYS\n",
    "        print('[' + split[0] + ' >= ' + str(split[1]) + ']: ' + str(infoGain) + ' (zero split entropy)')\n",
    "    else:\n",
    "        gainRatio = (HY - HYS) / HS\n",
    "        print('[' + split[0] + ' >= ' + str(split[1]) + ']: ' + str(gainRatio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Decision tree is not the most accurate classifier in general. However, it persists. This is largely due to its rumored interpretability: a data scientist can easily explain a tree to a non-data scientist. Build a tree from D3leaves.txt. Then manually convert your tree to a set of logic rules. Show the tree and the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0  10   1  1\n",
       "1  10   2  1\n",
       "2  10   3  1\n",
       "3   1   1  0\n",
       "4   1   3  1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3_df = pd.read_csv('./data/D3leaves.txt', sep='\\s+', names=['x1', 'x2', 'y'])\n",
    "d3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 >= 2\n",
      "|\n",
      "├──y = 1\n",
      "|\n",
      "└──x1 >= 10\n",
      "\t|\n",
      "\t├──y = 1\n",
      "\t|\n",
      "\t└──y = 0\n"
     ]
    }
   ],
   "source": [
    "d3_dtree = make_subtree(d3_df)\n",
    "print(d3_dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree created above can also be illustrated as:\n",
    "```\n",
    "           ┌─true───┤y = 1\n",
    "           │\n",
    "x2 >= 2 ───┤                     ┌─true───┤y = 1\n",
    "           │                     │\n",
    "           └─false──┤x1 >= 10├───┤\n",
    "                                 │\n",
    "                                 └─false──┤y = 0\n",
    "```\n",
    "This represents the following logical expression:\n",
    "```\n",
    "    y = 1 if x2 >= 2 or x1 >= 10\n",
    "    y = 0 otherwise\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. For this question only, make sure you DO NOT VISUALIZE the data sets or plot your tree’s decision boundary in the 2D x space. If your code does that, turn it off before proceeding. This is because you want to see your own reaction when trying to interpret a tree. You will get points no matter what your interpretation is. And we will ask you to visualize them in the next question anyway.\n",
    "- Build a decision tree on D1.txt. Show it to us in any format (e.g. could be a standard binary tree with nodes and arrows, and denote the rule at each leaf node; or as simple as plaintext output where each line represents a node with appropriate line number pointers to child nodes; whatever is convenient for you). Again, do not visualize the data set or the tree in the x input space. In real tasks you will not be able to visualize the whole high dimensional input space anyway, so we don’t want you to “cheat” here.\n",
    "- Look at your tree in the above format (remember, you should not visualize the 2D dataset or your tree’s decision boundary) and try to interpret the decision boundary in human understandable English.\n",
    "- Build a decision tree on D2.txt. Show it to us.\n",
    "- Try to interpret your D2 decision tree. Is it easy or possible to do so without visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 >= 0.201829\n",
      "|\n",
      "├──y = 1\n",
      "|\n",
      "└──y = 0\n"
     ]
    }
   ],
   "source": [
    "d1_df = pd.read_csv('./data/D1.txt', sep='\\s+', names=['x1', 'x2', 'y'])\n",
    "d1_dtree = make_subtree(d1_df)\n",
    "print(d1_dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree built on D1.txt is above. It has a depth of 2 and appears to be fairly simple. If x2 is greater than or equal to 0.201829, we predict y to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 >= 0.533076\n",
      "|\n",
      "├──x2 >= 0.228007\n",
      "\t|\n",
      "\t├──x2 >= 0.424906\n",
      "\t\t|\n",
      "\t\t├──y = 1\n",
      "\t\t|\n",
      "\t\t└──x1 >= 0.708127\n",
      "\t\t\t|\n",
      "\t\t\t├──y = 1\n",
      "\t\t\t|\n",
      "\t\t\t└──x2 >= 0.32625\n",
      "\t\t\t\t|\n",
      "\t\t\t\t├──x1 >= 0.595471\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t├──x1 >= 0.646007\n",
      "\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t├──y = 1\n",
      "\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t└──x2 >= 0.403494\n",
      "\t\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t\t├──y = 1\n",
      "\t\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t\t└──y = 0\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t└──y = 0\n",
      "\t\t\t\t|\n",
      "\t\t\t\t└──y = 0\n",
      "\t|\n",
      "\t└──x1 >= 0.887224\n",
      "\t\t|\n",
      "\t\t├──x2 >= 0.037708\n",
      "\t\t\t|\n",
      "\t\t\t├──x2 >= 0.082895\n",
      "\t\t\t\t|\n",
      "\t\t\t\t├──y = 1\n",
      "\t\t\t\t|\n",
      "\t\t\t\t└──x1 >= 0.960783\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t├──y = 1\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t└──y = 0\n",
      "\t\t\t|\n",
      "\t\t\t└──y = 0\n",
      "\t\t|\n",
      "\t\t└──x1 >= 0.850316\n",
      "\t\t\t|\n",
      "\t\t\t├──x2 >= 0.169053\n",
      "\t\t\t\t|\n",
      "\t\t\t\t├──y = 1\n",
      "\t\t\t\t|\n",
      "\t\t\t\t└──y = 0\n",
      "\t\t\t|\n",
      "\t\t\t└──y = 0\n",
      "|\n",
      "└──x2 >= 0.88635\n",
      "\t|\n",
      "\t├──x1 >= 0.041245\n",
      "\t\t|\n",
      "\t\t├──x1 >= 0.104043\n",
      "\t\t\t|\n",
      "\t\t\t├──y = 1\n",
      "\t\t\t|\n",
      "\t\t\t└──x1 >= 0.07642\n",
      "\t\t\t\t|\n",
      "\t\t\t\t├──y = 0\n",
      "\t\t\t\t|\n",
      "\t\t\t\t└──y = 1\n",
      "\t\t|\n",
      "\t\t└──y = 0\n",
      "\t|\n",
      "\t└──x2 >= 0.691474\n",
      "\t\t|\n",
      "\t\t├──x1 >= 0.254049\n",
      "\t\t\t|\n",
      "\t\t\t├──y = 1\n",
      "\t\t\t|\n",
      "\t\t\t└──x1 >= 0.191915\n",
      "\t\t\t\t|\n",
      "\t\t\t\t├──x2 >= 0.792752\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t├──y = 1\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t└──y = 0\n",
      "\t\t\t\t|\n",
      "\t\t\t\t└──x2 >= 0.864128\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t├──x1 >= 0.144781\n",
      "\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t├──y = 1\n",
      "\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t└──y = 0\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t└──y = 0\n",
      "\t\t|\n",
      "\t\t└──x2 >= 0.534979\n",
      "\t\t\t|\n",
      "\t\t\t├──x1 >= 0.426073\n",
      "\t\t\t\t|\n",
      "\t\t\t\t├──y = 1\n",
      "\t\t\t\t|\n",
      "\t\t\t\t└──x1 >= 0.409972\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t├──x1 >= 0.417579\n",
      "\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t├──y = 0\n",
      "\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t└──y = 1\n",
      "\t\t\t\t\t|\n",
      "\t\t\t\t\t└──x1 >= 0.393227\n",
      "\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t├──x1 >= 0.39583\n",
      "\t\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t\t├──y = 0\n",
      "\t\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t\t└──y = 1\n",
      "\t\t\t\t\t\t|\n",
      "\t\t\t\t\t\t└──y = 0\n",
      "\t\t\t|\n",
      "\t\t\t└──y = 0\n"
     ]
    }
   ],
   "source": [
    "d2_df = pd.read_csv('./data/D2.txt', sep='\\s+', names=['x1', 'x2', 'y'])\n",
    "d2_dtree = make_subtree(d2_df)\n",
    "print(d2_dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For D1.txt and D2.txt, do the following separately:\n",
    "- Produce a scatter plot of the data set.\n",
    "- Visualize your decision tree's decision boundary (or decision region, or some other ways to clearly visualize how your decision tree will make decisions in the feature space).\n",
    "\n",
    "Then discuss why the size of your decision trees on D1 and D2 differ. Relate this to the hypothesis space of our decision tree algorithm.\n",
    "\n",
    "\n",
    "7. We provide a data set Dbig.txt with 10000 labeled items. Caution: Dbig.txt is sorted.\n",
    "- You will randomly split Dbig.txt into a candidate training set of 8192 items and a test set (the rest). Do this by generating a random permutation, and split at 8192.\n",
    "- Generate a sequence of five nested training sets D32 ⊂ D128 ⊂ D512 ⊂ D2048 ⊂ D8192 from the candidate training set. The subscript n in Dn denotes training set size. The easiest way is to take the first n items from the (same) permutation above. This sequence simulates the real world situation where you obtain more and more training data.\n",
    "- For each Dn above, train a decision tree. Measure its test set error errn. Show three things in your\n",
    "answer: (1) List n, number of nodes in that tree, errn. (2) Plot n vs. errn. This is known as a learning\n",
    "curve (a single plot). (3) Visualize your decision trees’ decision boundary (five plots)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
